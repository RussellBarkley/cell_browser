{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97db90ad",
   "metadata": {},
   "source": [
    "# This is slow, rework this later.\n",
    "\n",
    "# Latent Averages → Nearest Real Images (with Robust Path Resolver)\n",
    "\n",
    "This notebook computes mean/median/geometric-median latents, decodes them,\n",
    "re-embeds via the encoder, finds nearest real images in latent space (Euclidean & Cosine),\n",
    "and shows a 3-panel interactive viewer.\n",
    "\n",
    "**Image root:** `D:\\Confocal_imaging_nuclei_tif\\MIST_Fused_Images\\ROIs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118012c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import os, math, glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "from tifffile import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Reshape, Conv2D, Conv2DTranspose,\n",
    "                                     ReLU, AveragePooling2D, Flatten)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "try:\n",
    "    for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "except Exception as e:\n",
    "    print(\"[!] Could not set GPU memory growth:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb594f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "   D:/Results/09052025_AE1M_Conv2DTranspose\\latents.h5 \n",
      "   D:/Results/09052025_AE1M_Conv2DTranspose\\encoder_weights.h5 \n",
      "   D:/Results/09052025_AE1M_Conv2DTranspose\\decoder_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Paths & knobs (EDIT IF NEEDED)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "RESULTS_DIR = r'D:/Results/09052025_AE1M_Conv2DTranspose'\n",
    "LATENTS_H5  = os.path.join(RESULTS_DIR, 'latents.h5')\n",
    "ENCODER_W   = os.path.join(RESULTS_DIR, 'encoder_weights.h5')\n",
    "DECODER_W   = os.path.join(RESULTS_DIR, 'decoder_weights.h5')\n",
    "\n",
    "INPUT_SHAPE = (256, 256, 1)\n",
    "LATENT_DIM  = 512\n",
    "\n",
    "CHUNK_ROWS   = 200_000   # rows per chunk when scanning /z\n",
    "K_NEIGHBORS  = 1000      # top-K neighbors to browse\n",
    "\n",
    "MEDIAN_SUBSAMPLE_N    = 200_000\n",
    "GEOMEDIAN_SUBSAMPLE_N = 100_000\n",
    "GEOMEDIAN_MAX_ITERS   = 200\n",
    "GEOMEDIAN_TOL         = 1e-6\n",
    "\n",
    "assert os.path.isfile(LATENTS_H5), f\"latents.h5 not found: {LATENTS_H5}\"\n",
    "assert os.path.isfile(ENCODER_W),  f\"encoder_weights.h5 not found: {ENCODER_W}\"\n",
    "assert os.path.isfile(DECODER_W),  f\"decoder_weights.h5 not found: {DECODER_W}\"\n",
    "print(\"Using:\\n  \", LATENTS_H5, \"\\n  \", ENCODER_W, \"\\n  \", DECODER_W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90b79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Robust path resolver\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "ROI_ROOT = r'D:\\\\Confocal_imaging_nuclei_tif\\\\MIST_Fused_Images\\\\ROIs'\n",
    "\n",
    "# Map any old prefix to ROI_ROOT if present\n",
    "PATH_REWRITE = [\n",
    "    (r'C:\\\\Users\\\\Work\\\\Desktop\\\\Github_repo\\\\cell_browser\\\\notebooks', ROI_ROOT),\n",
    "]\n",
    "\n",
    "CANDIDATE_ROOTS = [ROI_ROOT]\n",
    "\n",
    "def _normalize(p: str) -> str:\n",
    "    # Normalize slashes and collapse redundant separators\n",
    "    p = p.replace('/', '\\\\')\n",
    "    return os.path.normpath(p)\n",
    "\n",
    "def _try_extensions(path_wo_ext: str, exts=(\".tif\", \".tiff\", \".png\")):\n",
    "    for ext in exts:\n",
    "        q = path_wo_ext + ext\n",
    "        if os.path.exists(q):\n",
    "            return q\n",
    "    return None\n",
    "\n",
    "def resolve_path(p_in):\n",
    "    if p_in is None:\n",
    "        return None\n",
    "    p = p_in.decode('utf-8') if isinstance(p_in, (bytes, bytearray)) else str(p_in)\n",
    "    p = _normalize(p)\n",
    "\n",
    "    # direct\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "\n",
    "    # try rewrite prefixes\n",
    "    for old, new in PATH_REWRITE:\n",
    "        old_n = _normalize(old)\n",
    "        if p.lower().startswith(old_n.lower()):\n",
    "            rel = os.path.relpath(p, old_n)\n",
    "            q = _normalize(os.path.join(new, rel))\n",
    "            if os.path.exists(q):\n",
    "                return q\n",
    "            # try alt extensions\n",
    "            root, ext = os.path.splitext(q)\n",
    "            q2 = _try_extensions(root)\n",
    "            if q2:\n",
    "                return q2\n",
    "\n",
    "    # basename in candidate roots (direct)\n",
    "    base = os.path.basename(p)\n",
    "    if base:\n",
    "        for root in CANDIDATE_ROOTS:\n",
    "            q = _normalize(os.path.join(root, base))\n",
    "            if os.path.exists(q):\n",
    "                return q\n",
    "            # try alt extensions if base has one\n",
    "            b0, _ = os.path.splitext(q)\n",
    "            q2 = _try_extensions(b0)\n",
    "            if q2:\n",
    "                return q2\n",
    "\n",
    "    # last resort: recursive basename match (can be slower but robust)\n",
    "    if base:\n",
    "        for root in CANDIDATE_ROOTS:\n",
    "            hits = glob.glob(os.path.join(root, \"**\", base), recursive=True)\n",
    "            if hits:\n",
    "                return _normalize(hits[0])\n",
    "            # alt extensions\n",
    "            bname, _ = os.path.splitext(base)\n",
    "            for ext in (\".tif\", \".tiff\"):\n",
    "                hits = glob.glob(os.path.join(root, \"**\", bname + ext), recursive=True)\n",
    "                if hits:\n",
    "                    return _normalize(hits[0])\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63890436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Utilities: streaming mean, subsample, Weiszfeld geometric median\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def stream_mean_z(h5path, dataset='z', chunk_rows=100_000):\n",
    "    with h5py.File(h5path, 'r') as h5:\n",
    "        z = h5[dataset]\n",
    "        N, D = z.shape\n",
    "        acc = np.zeros(D, dtype=np.float64)\n",
    "        for start in range(0, N, chunk_rows):\n",
    "            end = min(start + chunk_rows, N)\n",
    "            acc += z[start:end].astype(np.float64).sum(axis=0)\n",
    "        return (acc / N).astype(np.float32)\n",
    "\n",
    "def random_subsample(h5path, n_wanted, dataset='z', seed=42, chunk=65536):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    with h5py.File(h5path, 'r') as h5:\n",
    "        z = h5[dataset]\n",
    "        N, D = z.shape\n",
    "        M = min(n_wanted, N)\n",
    "        idx = rng.choice(N, size=M, replace=False)\n",
    "        idx.sort()\n",
    "        out = np.empty((M, D), dtype=np.float32)\n",
    "        pos = 0\n",
    "        while pos < M:\n",
    "            end = min(pos + chunk, M)\n",
    "            out[pos:end] = z[idx[pos:end]]\n",
    "            pos = end\n",
    "        return out\n",
    "\n",
    "def weiszfeld_geometric_median(points, x0=None, max_iters=200, tol=1e-6, eps=1e-12):\n",
    "    P = np.asarray(points, dtype=np.float64)\n",
    "    x = np.median(P, axis=0) if x0 is None else np.asarray(x0, dtype=np.float64)\n",
    "    for it in range(1, max_iters + 1):\n",
    "        diff = P - x\n",
    "        dist = np.linalg.norm(diff, axis=1)\n",
    "        zero_mask = dist < eps\n",
    "        if np.any(zero_mask):\n",
    "            return P[zero_mask][0].astype(np.float32), it\n",
    "        w = 1.0 / np.maximum(dist, eps)\n",
    "        x_new = (P * w[:, None]).sum(axis=0) / w.sum()\n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            return x_new.astype(np.float32), it\n",
    "        x = x_new\n",
    "    return x.astype(np.float32), max_iters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92aaa751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Latents: N=1,061,277, D=512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m D == LATENT_DIM, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLATENT_DIM mismatch: z has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLATENT_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m z_mean = stream_mean_z(LATENTS_H5, \u001b[33m'\u001b[39m\u001b[33mz\u001b[39m\u001b[33m'\u001b[39m, CHUNK_ROWS)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m Z_med_sample = \u001b[43mrandom_subsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLATENTS_H5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMEDIAN_SUBSAMPLE_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m z_median = np.median(Z_med_sample, axis=\u001b[32m0\u001b[39m).astype(np.float32)\n\u001b[32m     12\u001b[39m Z_geo_sample = random_subsample(LATENTS_H5, GEOMEDIAN_SUBSAMPLE_N, \u001b[33m'\u001b[39m\u001b[33mz\u001b[39m\u001b[33m'\u001b[39m, seed=\u001b[32m123\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mrandom_subsample\u001b[39m\u001b[34m(h5path, n_wanted, dataset, seed, chunk)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m pos < M:\n\u001b[32m     25\u001b[39m     end = \u001b[38;5;28mmin\u001b[39m(pos + chunk, M)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     out[pos:end] = \u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m     pos = end\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Program_Files\\Anaconda3_2024.10-1\\envs\\cell_browser\\Lib\\site-packages\\h5py\\_hl\\dataset.py:820\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Representatives: mean / median / geometric median\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "with h5py.File(LATENTS_H5, 'r') as h5:\n",
    "    N, D = h5['z'].shape\n",
    "print(f\"[i] Latents: N={N:,}, D={D}\")\n",
    "assert D == LATENT_DIM, f\"LATENT_DIM mismatch: z has {D}, expected {LATENT_DIM}\"\n",
    "\n",
    "z_mean = stream_mean_z(LATENTS_H5, 'z', CHUNK_ROWS)\n",
    "Z_med_sample = random_subsample(LATENTS_H5, MEDIAN_SUBSAMPLE_N, 'z')\n",
    "z_median = np.median(Z_med_sample, axis=0).astype(np.float32)\n",
    "Z_geo_sample = random_subsample(LATENTS_H5, GEOMEDIAN_SUBSAMPLE_N, 'z', seed=123)\n",
    "z_geomed, iters = weiszfeld_geometric_median(Z_geo_sample, x0=z_median,\n",
    "                                            max_iters=GEOMEDIAN_MAX_ITERS,\n",
    "                                            tol=GEOMEDIAN_TOL)\n",
    "print(\"[i] Representatives ready (mean/median/geomed).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Decoder → decode reps\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def build_decoder(latent_dim=512):\n",
    "    latent_in = Input((latent_dim,), name='z_sampling')\n",
    "    x = Dense(16 * 16 * 128)(latent_in)\n",
    "    x = Reshape((16, 16, 128))(x)\n",
    "    for filters in [128, 64, 32, 16]:\n",
    "        x = Conv2DTranspose(filters, 3, strides=2, padding='same')(x); x = ReLU()(x)\n",
    "        x = Conv2D(filters, 3, padding='same')(x); x = ReLU()(x)\n",
    "    decoded = Conv2D(1, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "    return Model(latent_in, decoded, name='decoder')\n",
    "\n",
    "decoder = build_decoder(LATENT_DIM)\n",
    "decoder.load_weights(DECODER_W)\n",
    "decoder.trainable = False\n",
    "\n",
    "def decode_vec(v):\n",
    "    arr = np.asarray(v, dtype=np.float32)[None, ...]\n",
    "    out = decoder.predict(arr, batch_size=1, verbose=0)[0, ..., 0]\n",
    "    return np.clip(out, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "img_mean   = decode_vec(z_mean)\n",
    "img_median = decode_vec(z_median)\n",
    "img_geomed = decode_vec(z_geomed)\n",
    "print(\"[i] Decoded mean/median/geomedian images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Encoder → embed decoded images\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def build_encoder(input_shape=(256,256,1), latent_dim=512):\n",
    "    inp = Input(input_shape, name='encoder_input')\n",
    "    x = inp\n",
    "    for filters in [16, 32, 64, 128]:\n",
    "        x = Conv2D(filters, 3, padding='same')(x); x = ReLU()(x)\n",
    "        x = Conv2D(filters, 3, padding='same')(x); x = ReLU()(x)\n",
    "        x = AveragePooling2D(pool_size=2, strides=2, padding='valid')(x)\n",
    "    flat = Flatten()(x)\n",
    "    z = Dense(latent_dim, name='z')(flat)\n",
    "    return Model(inp, z, name='encoder')\n",
    "\n",
    "encoder = build_encoder(INPUT_SHAPE, LATENT_DIM)\n",
    "encoder.load_weights(ENCODER_W)\n",
    "encoder.trainable = False\n",
    "\n",
    "def encode_image(img01):\n",
    "    arr = img01.astype(np.float32)[None, ..., None]\n",
    "    return encoder.predict(arr, batch_size=1, verbose=0)[0].astype(np.float32)\n",
    "\n",
    "q_mean   = encode_image(img_mean)\n",
    "q_median = encode_image(img_median)\n",
    "q_geomed = encode_image(img_geomed)\n",
    "print(\"[i] Encoded average images → latent vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Nearest neighbors (streaming Top-K) + filename resolution\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "def _merge_topk(idxs_a, dists_a, idxs_b, dists_b, K):\n",
    "    if idxs_a.size == 0:\n",
    "        idxs, dists = idxs_b, dists_b\n",
    "    else:\n",
    "        idxs = np.concatenate([idxs_a, idxs_b])\n",
    "        dists = np.concatenate([dists_a, dists_b])\n",
    "    if idxs.size <= K:\n",
    "        return idxs, dists\n",
    "    keep = np.argpartition(dists, K-1)[:K]\n",
    "    idxs, dists = idxs[keep], dists[keep]\n",
    "    order = np.argsort(dists)\n",
    "    return idxs[order], dists[order]\n",
    "\n",
    "def topk_euclidean(h5path, q, K=1000, chunk_rows=200_000):\n",
    "    q = q.astype(np.float32)\n",
    "    idxs_keep = np.empty(0, dtype=np.int64)\n",
    "    dists_keep = np.empty(0, dtype=np.float32)\n",
    "    with h5py.File(h5path, 'r') as h5:\n",
    "        Z = h5['z']\n",
    "        N, D = Z.shape\n",
    "        for s in range(0, N, chunk_rows):\n",
    "            e = min(s + chunk_rows, N)\n",
    "            chunk = Z[s:e].astype(np.float32)\n",
    "            diffs = chunk - q\n",
    "            d2 = np.einsum('ij,ij->i', diffs, diffs)\n",
    "            k_local = min(K, d2.size)\n",
    "            loc_sel = np.argpartition(d2, k_local-1)[:k_local]\n",
    "            idxs_b = (s + loc_sel).astype(np.int64)\n",
    "            dists_b = d2[loc_sel]\n",
    "            idxs_keep, dists_keep = _merge_topk(idxs_keep, dists_keep, idxs_b, dists_b, K)\n",
    "    dists_keep = np.sqrt(dists_keep)\n",
    "    order = np.argsort(dists_keep)\n",
    "    return idxs_keep[order], dists_keep[order]\n",
    "\n",
    "def topk_cosine(h5path, q, K=1000, chunk_rows=200_000, eps=1e-8):\n",
    "    q = q.astype(np.float32)\n",
    "    q_norm = max(np.linalg.norm(q), eps)\n",
    "    idxs_keep = np.empty(0, dtype=np.int64)\n",
    "    cd_keep = np.empty(0, dtype=np.float32)\n",
    "    with h5py.File(h5path, 'r') as h5:\n",
    "        Z = h5['z']\n",
    "        N, D = Z.shape\n",
    "        for s in range(0, N, chunk_rows):\n",
    "            e = min(s + chunk_rows, N)\n",
    "            chunk = Z[s:e].astype(np.float32)\n",
    "            z_norms = np.linalg.norm(chunk, axis=1)\n",
    "            dots = chunk @ q\n",
    "            sims = dots / (np.maximum(z_norms, eps) * q_norm)\n",
    "            cdist = 1.0 - sims\n",
    "            k_local = min(K, cdist.size)\n",
    "            loc_sel = np.argpartition(cdist, k_local-1)[:k_local]\n",
    "            idxs_b = (s + loc_sel).astype(np.int64)\n",
    "            cd_b = cdist[loc_sel]\n",
    "            idxs_keep, cd_keep = _merge_topk(idxs_keep, cd_keep, idxs_b, cd_b, K)\n",
    "    order = np.argsort(cd_keep)\n",
    "    return idxs_keep[order], cd_keep[order]\n",
    "\n",
    "def get_filenames(h5path, indices, dataset='filenames'):\n",
    "    idx_in = np.asarray(indices, dtype=np.int64)\n",
    "    order = np.argsort(idx_in)\n",
    "    idx_sorted = idx_in[order]\n",
    "    with h5py.File(h5path, 'r') as h5:\n",
    "        ds = h5[dataset]\n",
    "        fn_sorted = ds[idx_sorted]\n",
    "    fn_sorted = [f.decode('utf-8') if isinstance(f, (bytes, bytearray)) else str(f)\n",
    "                 for f in fn_sorted]\n",
    "    inv = np.empty_like(order)\n",
    "    inv[order] = np.arange(order.size)\n",
    "    return list(np.asarray(fn_sorted, dtype=object)[inv])\n",
    "\n",
    "queries = {\n",
    "    'Arithmetic mean': encode_image(img_mean),\n",
    "    'Component-wise median': encode_image(img_median),\n",
    "    'Geometric median': encode_image(img_geomed),\n",
    "}\n",
    "\n",
    "neighbors = {'Euclidean': {}, 'Cosine': {}}\n",
    "for key, q in queries.items():\n",
    "    idx_e, d_e = topk_euclidean(LATENTS_H5, q, K=K_NEIGHBORS, chunk_rows=CHUNK_ROWS)\n",
    "    neighbors['Euclidean'][key] = {'idx': idx_e, 'val': d_e}\n",
    "    idx_c, d_c = topk_cosine(LATENTS_H5, q, K=K_NEIGHBORS, chunk_rows=CHUNK_ROWS)\n",
    "    neighbors['Cosine'][key] = {'idx': idx_c, 'val': d_c}\n",
    "print(\"[i] Nearest neighbors computed.\")\n",
    "\n",
    "filenames_cache = {'Euclidean': {}, 'Cosine': {}}\n",
    "for metric in ['Euclidean', 'Cosine']:\n",
    "    for key in queries.keys():\n",
    "        idxs = neighbors[metric][key]['idx']\n",
    "        raw  = get_filenames(LATENTS_H5, idxs)\n",
    "        resolved = [resolve_path(p) for p in raw]\n",
    "        missing = sum(1 for p in resolved if p is None)\n",
    "        print(f\"[{metric} · {key}] resolved={len(resolved)} missing={missing}\")\n",
    "        filenames_cache[metric][key] = resolved\n",
    "print(\"[i] Filenames cached & resolved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Sanity check: show a few resolved paths\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "for metric in ['Euclidean', 'Cosine']:\n",
    "    for key in ['Arithmetic mean']:\n",
    "        sample = filenames_cache[metric][key][:5]\n",
    "        print(metric, key, 'sample resolved paths:')\n",
    "        for p in sample:\n",
    "            print('  ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Interactive 3-panel viewer (single live figure)\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "plt.close('all'); plt.ioff()\n",
    "\n",
    "metric_toggle = W.ToggleButtons(options=['Euclidean', 'Cosine'], value='Euclidean', description='Metric:')\n",
    "rank_slider   = W.IntSlider(value=0, min=0, max=min(K_NEIGHBORS, 1000)-1, step=1, description='Rank:')\n",
    "rng_slider    = W.IntRangeSlider(value=[0, 255], min=0, max=255, step=1, description='Range:')\n",
    "btn_auto      = W.Button(description='Auto (2–98%)')\n",
    "btn_reset     = W.Button(description='Reset 0–255')\n",
    "skip_missing  = W.Checkbox(value=True, description='Skip missing')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "keys = ['Arithmetic mean', 'Component-wise median', 'Geometric median']\n",
    "im_artists = []\n",
    "for ax in axes:\n",
    "    im = ax.imshow(np.zeros((256,256), dtype=np.float32), cmap='gray', vmin=0, vmax=1)\n",
    "    im_artists.append(im)\n",
    "\n",
    "image_cache = {}\n",
    "def load_img_cached(path):\n",
    "    if not path or not os.path.exists(path):\n",
    "        return None\n",
    "    arr = image_cache.get(path)\n",
    "    if arr is None:\n",
    "        arr = imread(path).astype(np.float32)\n",
    "        if arr.ndim == 3:\n",
    "            arr = arr[..., 0]\n",
    "        arr = np.clip(arr / 255.0, 0.0, 1.0)\n",
    "        image_cache[path] = arr\n",
    "    return arr\n",
    "\n",
    "def _first_available(metric, key, start_k):\n",
    "    # find the first rank >= start_k that has a resolvable file\n",
    "    idxs = filenames_cache[metric][key]\n",
    "    for kk in range(start_k, min(len(idxs), K_NEIGHBORS)):\n",
    "        if idxs[kk] and os.path.exists(idxs[kk]):\n",
    "            return kk\n",
    "    return start_k\n",
    "\n",
    "def update_view(*_):\n",
    "    metric = metric_toggle.value\n",
    "    k = rank_slider.value\n",
    "    if skip_missing.value:\n",
    "        # ensure k refers to available files for each panel\n",
    "        k = min(_first_available(metric, key, k) for key in keys)\n",
    "        rank_slider.value = k\n",
    "\n",
    "    lo, hi = rng_slider.value\n",
    "    lo01, hi01 = lo/255.0, hi/255.0\n",
    "\n",
    "    for ax, key, im_artist in zip(axes, keys, im_artists):\n",
    "        path  = filenames_cache[metric][key][k]\n",
    "        val_k = neighbors[metric][key]['val'][k]\n",
    "        img01 = load_img_cached(path)\n",
    "        if img01 is None:\n",
    "            img01 = np.zeros((256, 256), dtype=np.float32)\n",
    "            base = os.path.basename(path) if path else \"(missing)\"\n",
    "            title_val = (f\"cos sim=—\" if metric == 'Cosine' else \"L2=—\")\n",
    "            ax.set_title(f\"{key}\\nrank {k} • {title_val}\\nMISSING • {base}\")\n",
    "        else:\n",
    "            if metric == 'Cosine':\n",
    "                sim = 1.0 - float(val_k)\n",
    "                ax.set_title(f\"{key}\\nrank {k} • cos sim={sim:.4f}\\n{os.path.basename(path)}\")\n",
    "            else:\n",
    "                ax.set_title(f\"{key}\\nrank {k} • L2={float(val_k):.4f}\\n{os.path.basename(path)}\")\n",
    "        im_artist.set_data(img01)\n",
    "        im_artist.set_clim(lo01, hi01)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def on_auto_click(_):\n",
    "    metric = metric_toggle.value\n",
    "    k = rank_slider.value\n",
    "    path = filenames_cache[metric]['Arithmetic mean'][k]\n",
    "    arr  = load_img_cached(path)\n",
    "    if arr is None:\n",
    "        rng_slider.value = (0, 255)\n",
    "    else:\n",
    "        lo = int(np.clip(round(np.percentile(arr, 2)  * 255.0), 0, 255))\n",
    "        hi = int(np.clip(round(np.percentile(arr, 98) * 255.0), 0, 255))\n",
    "        if lo >= hi:\n",
    "            lo, hi = 0, 255\n",
    "        rng_slider.value = (lo, hi)\n",
    "    update_view()\n",
    "\n",
    "def on_reset_click(_):\n",
    "    rng_slider.value = (0, 255)\n",
    "    update_view()\n",
    "\n",
    "metric_toggle.observe(update_view, names='value')\n",
    "rank_slider.observe(update_view, names='value')\n",
    "rng_slider.observe(update_view, names='value')\n",
    "btn_auto.on_click(on_auto_click)\n",
    "btn_reset.on_click(on_reset_click)\n",
    "\n",
    "controls = W.VBox([\n",
    "    metric_toggle,\n",
    "    rank_slider,\n",
    "    rng_slider,\n",
    "    W.HBox([btn_auto, btn_reset, skip_missing]),\n",
    "])\n",
    "\n",
    "display(W.VBox([controls, fig.canvas]))\n",
    "update_view()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
